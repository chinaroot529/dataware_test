#!/usr/bin/env python3
"""
Question Similarity Recommender (reâ€‘worked)
=========================================
* Cleans HTML & entities
* Truncates long texts to 7.5â€¯k tokens (â‰ˆâ€¯textâ€‘embeddingâ€‘3â€‘large limit)
* Generates embeddings with exponential backâ€‘off on rateâ€‘limit errors
* L2â€‘normalises embeddings â†’ innerâ€‘product â‰¡ cosine similarity
* Uses **question_id** (not dataframe row) as Faiss vector ID
* Persists to three separate files (.feather / .npy / .faiss) for memory efficiency
* Compatible with Python â‰¥3.9

Install deps (oneâ€‘off):
    pip install pandas numpy tqdm faiss-cpu openai tiktoken beautifulsoup4 lxml

Run:
    export OPENAI_API_KEY="skâ€‘â€¦"
    python question_similarity_recommender.py /path/to/questions.csv  # trains & demo
"""

from __future__ import annotations
import os, re, html, json, time, random, sys, warnings, pickle, pathlib
from typing import List, Dict, Optional

import numpy as np
import pandas as pd
from bs4 import BeautifulSoup
from tqdm import tqdm
import faiss
from openai import OpenAI, OpenAIError

try:
    import tiktoken
    _enc = tiktoken.encoding_for_model("text-embedding-3-large")
except ImportError:
    _enc = None
    warnings.warn("tiktoken not installed â€“ token count truncation disabled", RuntimeWarning)

# ---------------------------------------------------------------------------
# Helper functions
# ---------------------------------------------------------------------------

def _num_tokens(text: str) -> int:
    if _enc is None:
        return len(text.split())  # rough fallback
    return len(_enc.encode(text))

TOKEN_LIMIT = 8000  # hard limit 8192 â€“ leave a buffer
EMBED_DIM   = 3072  # textâ€‘embeddingâ€‘3â€‘large dimension

# ---------------------------------------------------------------------------
class QuestionSimilarityRecommender:
    """Semantic similarity recommender for exam questions."""

    def __init__(self, csv_path: str, api_key: Optional[str] = None):
        self.csv_path = pathlib.Path(csv_path)
        self.api_key  = api_key or os.getenv("OPENAI_API_KEY")
        if not self.api_key:
            raise RuntimeError("OpenAI API key not provided. Set OPENAI_API_KEY env or pass api_key param.")
        self.client    = OpenAI(api_key=self.api_key)
        self.df: pd.DataFrame = pd.DataFrame()
        self.emb: np.ndarray  = np.empty((0, EMBED_DIM), dtype="float32")
        self.index: faiss.IndexIDMap | None = None

    # ---------------------------------------------------------------------
    # Data handling
    # ---------------------------------------------------------------------
    def _clean_html(self, text: str) -> str:
        text = html.unescape(str(text))
        text = re.sub(r"<!--.*?-->", " ", text, flags=re.S)
        soup = BeautifulSoup(text, "lxml")
        text = soup.get_text(" ")
        text = re.sub(r"\s+", " ", text)
        return text.strip()

    def _make_text(self, row: pd.Series) -> str:
        parts = [self._clean_html(row.get("content", ""))]
        # only choice questions - check for various choice question type IDs
        type_id = row.get("type_id")
        if pd.notna(type_id) and (int(type_id) in {1, 2} or str(type_id).startswith('10')):
            options = row.get("options", "")
            if options:
                parts.append(self._clean_html(options))
        text = " ".join(p for p in parts if p)
        if _enc and _num_tokens(text) > TOKEN_LIMIT:
            cut = _enc.decode(_enc.encode(text)[:7500])
            text = cut + " [TRUNCATED]"
        return text

    def load_data(self) -> None:
        print("ğŸ“Š  Loading CSV data â€¦")
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æœ‰header
        sample = pd.read_csv(self.csv_path, nrows=1)
        if 'question_id' in sample.columns:
            # æœ‰headerçš„CSVæ–‡ä»¶
            self.df = pd.read_csv(self.csv_path)
        else:
            # æ²¡æœ‰headerçš„CSVæ–‡ä»¶
            self.df = pd.read_csv(self.csv_path, header=None, engine="python", on_bad_lines="skip", quoting=3)

        # å¦‚æœæ²¡æœ‰headerï¼Œéœ€è¦é‡å‘½ååˆ—
        if 'question_id' not in self.df.columns:
            colmap = {
                0: "question_id", 1: "subject_id", 3: "type_id", 5: "grade_id", 8: "difficulty_id",
                10: "content", 12: "answer", 13: "answer_list", 14: "analysis"
            }
            # rename columns if present
            self.df.rename(columns={i: name for i, name in colmap.items() if i < len(self.df.columns)}, inplace=True)
        # attempt to find options column (contains 'A.' etc.)
        for col in self.df.columns[::-1]:
            sample = str(self.df[col].iloc[0])
            if re.search(r"[A-D][\.ã€]", sample) or "â‘ " in sample:
                self.df.rename(columns={col: "options"}, inplace=True)
                break
        print(f"âœ…  Loaded {len(self.df):,} rows â€“ fields: {list(self.df.columns)[:15]}â€¦")

    # ---------------------------------------------------------------------
    # Embeddings with backâ€‘off
    # ---------------------------------------------------------------------
    def _embed_batch(self, texts: List[str]) -> List[List[float]]:
        for attempt in range(6):
            try:
                resp = self.client.embeddings.create(model="text-embedding-3-large", input=texts, encoding_format="float")
                return [x.embedding for x in resp.data]
            except OpenAIError as e:
                wait = 2 ** attempt + random.random()
                print(f"âš ï¸  OpenAI error ({e}); retry in {wait:.1f}s â€¦")
                time.sleep(wait)
        raise RuntimeError("Failed to get embeddings after retries")

    def build_embeddings(self, batch_size: int = 64) -> None:
        texts, keep_rows = [], []
        for idx, row in self.df.iterrows():
            t = self._make_text(row)
            if t:
                texts.append(t)
                keep_rows.append(idx)
        self.df = self.df.loc[keep_rows].reset_index(drop=True)

        print(f"ğŸ§   Generating embeddings for {len(texts):,} texts â€¦")
        embs: List[List[float]] = []
        for i in tqdm(range(0, len(texts), batch_size)):
            embs.extend(self._embed_batch(texts[i:i + batch_size]))
        self.emb = np.asarray(embs, dtype="float32")
        faiss.normalize_L2(self.emb)
        print("âœ…  Embeddings shape:", self.emb.shape)

    # ---------------------------------------------------------------------
    # Faiss index
    # ---------------------------------------------------------------------
    def build_index(self) -> None:
        question_id_values = self.df["question_id"].fillna(-1).astype("int64").values
        index_flat = faiss.IndexFlatIP(EMBED_DIM)
        self.index = faiss.IndexIDMap(index_flat)
        self.index.add_with_ids(self.emb, question_id_values)
        print(f"ğŸ”  Faiss index built â€“ {self.index.ntotal:,} vectors")

    # ---------------------------------------------------------------------
    # Persistence
    # ---------------------------------------------------------------------
    def save(self, prefix: str = "question_sim") -> None:
        print("ğŸ’¾  Saving model files â€¦")
        self.df.to_feather(f"{prefix}.feather")
        np.save(f"{prefix}_emb.npy", self.emb)
        faiss.write_index(self.index, f"{prefix}.faiss")
        print("âœ…  Saved:", f"{prefix}.feather", f"{prefix}_emb.npy", f"{prefix}.faiss")

    def load(self, prefix: str = "question_sim") -> None:
        print("ğŸ“‚  Loading model files â€¦")
        self.df  = pd.read_feather(f"{prefix}.feather")
        self.emb = np.load(f"{prefix}_emb.npy")
        self.index = faiss.read_index(f"{prefix}.faiss")
        print("âœ…  Model loaded â€“", self.df.shape, self.emb.shape)

    # ---------------------------------------------------------------------
    # Query
    # ---------------------------------------------------------------------
    def _embed_query(self, text: str) -> np.ndarray:
        vec = np.asarray(self._embed_batch([text])[0], dtype="float32")
        faiss.normalize_L2(vec.reshape(1, -1))
        return vec

    def search(self, *, query_text: str | None = None, query_id: int | None = None,
               top_k: int = 10, filters: Optional[Dict[str, int]] = None) -> List[Dict]:
        if query_text is None and query_id is None:
            raise ValueError("Provide query_text or query_id")
        if query_text:
            qvec = self._embed_query(query_text).reshape(1, -1)
        else:
            row = self.df[self.df["question_id"] == query_id]
            if row.empty:
                raise ValueError("query_id not found")
            idx = row.index[0]
            qvec = self.emb[idx:idx + 1]
        D, I = self.index.search(qvec, top_k * 3)
        out = []
        for score, qid in zip(D[0], I[0]):
            if qid == -1 or (query_id is not None and qid == query_id):
                continue
            row = self.df[self.df["question_id"] == qid].iloc[0]
            if filters and any(row.get(k) != v for k, v in filters.items() if k in row):
                continue
            out.append({
                "question_id": int(qid),
                "similarity": float(score),
                "content": row.get("content", "")[:120],
                "type_id": row.get("type_id"),
                "grade_id": row.get("grade_id"),
                "subject_id": row.get("subject_id"),
            })
            if len(out) >= top_k:
                break
        return out

    # ---------------------------------------------------------------------
    # Incremental update
    # ---------------------------------------------------------------------
    def add_questions(self, new_questions: List[Dict], update_files: bool = True) -> int:
        """
        å¢é‡æ·»åŠ æ–°é¢˜ç›®åˆ°ç°æœ‰æ¨¡å‹
        
        Parameters:
        -----------
        new_questions : List[Dict]
            æ–°é¢˜ç›®åˆ—è¡¨ï¼Œæ¯ä¸ªdictåŒ…å«question_id, content, optionsç­‰å­—æ®µ
        update_files : bool, default True
            æ˜¯å¦æ›´æ–°ä¿å­˜çš„æ¨¡å‹æ–‡ä»¶
            
        Returns:
        --------
        int : å®é™…æ·»åŠ çš„é¢˜ç›®æ•°é‡
        """
        if not new_questions:
            return 0
            
        print(f"ğŸ“¥  Processing {len(new_questions)} new questions...")
        
        # 1. å»é‡ï¼šæ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        existing_question_ids = set(self.df["question_id"].astype(str))
        unique_questions = [
            q for q in new_questions 
            if str(q.get("question_id", "")) not in existing_question_ids
        ]
        
        if not unique_questions:
            print("â„¹ï¸  All questions already exist in dataset")
            return 0
            
        print(f"â•  Adding {len(unique_questions)} unique questions (filtered {len(new_questions) - len(unique_questions)} duplicates)")
        
        # 2. è½¬æ¢ä¸ºDataFrameå¹¶æ‹¼æ¥
        new_df = pd.DataFrame(unique_questions)
        original_len = len(self.df)
        self.df = pd.concat([self.df, new_df], ignore_index=True)
        
        # 3. ä¸ºæ–°é¢˜ç›®ç”ŸæˆåµŒå…¥
        new_texts = []
        for idx in range(original_len, len(self.df)):
            row = self.df.iloc[idx]
            text = self._make_text(row)
            if text:
                new_texts.append(text)
            else:
                # å¦‚æœæ–‡æœ¬ä¸ºç©ºï¼Œä»DataFrameä¸­ç§»é™¤è¿™è¡Œ
                self.df = self.df.drop(idx).reset_index(drop=True)
                
        if not new_texts:
            print("âš ï¸  No valid text found in new questions")
            return 0
            
        print(f"ğŸ§   Generating embeddings for {len(new_texts)} new texts...")
        new_embs = []
        for i in tqdm(range(0, len(new_texts), 64)):
            new_embs.extend(self._embed_batch(new_texts[i:i + 64]))
        
        new_emb_array = np.asarray(new_embs, dtype="float32")
        faiss.normalize_L2(new_emb_array)
        
        # 4. æ›´æ–°åµŒå…¥çŸ©é˜µ
        self.emb = np.vstack([self.emb, new_emb_array])
        
        # 5. é‡å»ºç´¢å¼•ï¼ˆåŒ…å«æ–°çš„å‘é‡ï¼‰
        print("ğŸ”„  Rebuilding Faiss index with new vectors...")
        question_id_values = self.df["question_id"].fillna(-1).astype("int64").values
        index_flat = faiss.IndexFlatIP(EMBED_DIM)
        self.index = faiss.IndexIDMap(index_flat)
        self.index.add_with_ids(self.emb, question_id_values)
        
        # 6. å¯é€‰ï¼šæ›´æ–°ä¿å­˜çš„æ–‡ä»¶
        if update_files:
            self.save()
            
        added_count = len(unique_questions)
        print(f"âœ…  Successfully added {added_count} questions. Total: {len(self.df)}")
        return added_count

    def update_from_csv(self, csv_path: str, update_files: bool = True) -> int:
        """
        ä»CSVæ–‡ä»¶å¢é‡æ›´æ–°æ¨¡å‹
        
        Parameters:
        -----------
        csv_path : str
            æ–°æ•°æ®çš„CSVæ–‡ä»¶è·¯å¾„
        update_files : bool, default True
            æ˜¯å¦æ›´æ–°ä¿å­˜çš„æ¨¡å‹æ–‡ä»¶
            
        Returns:
        --------
        int : å®é™…æ·»åŠ çš„é¢˜ç›®æ•°é‡
        """
        print(f"ğŸ“‚  Loading new data from {csv_path}")
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æœ‰header
        sample = pd.read_csv(csv_path, nrows=1)
        if 'question_id' in sample.columns:
            new_df = pd.read_csv(csv_path)
        else:
            new_df = pd.read_csv(csv_path, header=None, engine="python", on_bad_lines="skip", quoting=3)
            # é‡å‘½ååˆ—
            colmap = {
                0: "question_id", 1: "subject_id", 3: "type_id", 5: "grade_id", 8: "difficulty_id",
                10: "content", 12: "answer", 13: "answer_list", 14: "analysis"
            }
            new_df.rename(columns={i: name for i, name in colmap.items() if i < len(new_df.columns)}, inplace=True)
            
            # å°è¯•è¯†åˆ«optionsåˆ—
            for col in new_df.columns[::-1]:
                sample = str(new_df[col].iloc[0])
                if re.search(r"[A-D][\.ã€]", sample) or "â‘ " in sample:
                    new_df.rename(columns={col: "options"}, inplace=True)
                    break
        
        # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
        new_questions = new_df.to_dict('records')
        return self.add_questions(new_questions, update_files)

    # ---------------------------------------------------------------------
    # Endâ€‘toâ€‘end train
    # ---------------------------------------------------------------------
    def train(self):
        self.load_data()
        self.build_embeddings()
        self.build_index()

# ---------------------------------------------------------------------------
# CLI entry
# ---------------------------------------------------------------------------
if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("Usage: python question_similarity_recommender.py questions.csv [--demo]")
        sys.exit(1)

    csv_file = sys.argv[1]
    demo     = "--demo" in sys.argv

    rec = QuestionSimilarityRecommender(csv_file)
    rec.train()
    rec.save()

    if demo:
        rnd_id = int(rec.df["question_id"].sample(1).iloc[0])
        print("\nQuery ID:", rnd_id)
        similar = rec.search(query_id=rnd_id, top_k=5)
        print(json.dumps(similar, ensure_ascii=False, indent=2))
